# TL-DR-Extraction-from-Reddit-Threads-with-Neural-Networks
This project addresses the challenge of text summarization in Natural Language Processing (NLP), focusing on generating concise TL;DR (too long; did not read) summaries for Reddit posts. It proposes implementing neural network models by utilizing the 'Summarize from Feedback' dataset from Hugging Face, comprising around 179k Reddit posts and corresponding summaries. The project begins with baseline models employing extractive methods and then advances to sequence-to-sequence models with attention mechanisms. It also explores pre-trained language models for enhanced contextual understanding. The effectiveness of the developed models will be quantitatively evaluated using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score, encompassing ROUGE-1, ROUGE-2, and ROUGE-L metrics.

The findings suggest that BART-large-cnn performs the best across all three ROUGE metrics, followed closely by BART-large. This indicates that the BART-large model, specifically when fine-tuned on a relevant dataset (like CNN/Daily Mail for summarization tasks), performs well at summarizing texts compared to the base and smaller T5 models. The T5 models perform comparably, with the T5-base scoring slightly higher than T5-small on ROUGE-1 and ROUGEL but lower on ROUGE-2.

![alt text]([http://url/to/img.png](https://github.com/patilurjit/TL-DR-Extraction-from-Reddit-Threads-with-Neural-Networks/blob/main/Result%20Plot.png)https://github.com/patilurjit/TL-DR-Extraction-from-Reddit-Threads-with-Neural-Networks/blob/main/Result%20Plot.png)
